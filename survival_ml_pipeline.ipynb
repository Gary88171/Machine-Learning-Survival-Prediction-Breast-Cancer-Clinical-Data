import pandas as pd

# 1. Load the dataset from your CSV file
df = pd.read_csv("Breast_Cancer.csv")

# 2. Look at the first 5 rows to make sure it loaded correctly
df.head()

# 3. Check for missing values and see what types of data (numbers vs words) we have
df.info()

# 4. See how many patients are "Alive" vs "Dead" (checking for class imbalance)
df["Status"].value_counts()

# 5. Set our 'Target' (what we want to predict) and 'Features' (the data we use to predict it)
y = df["Status"]
X = df.drop("Status", axis=1)

# 6. Convert words (like "Stage 2") into numbers so the math can work
X_encoded = pd.get_dummies(X)

# 7. Turn our target into 1s and 0s (Computer likes 1 for 'Alive', 0 for 'Dead')
y_encoded = y.map({"Alive": 1, "Dead": 0})

from sklearn.model_selection import train_test_split

# 8. Split the data: 80% for the model to study (Train), 20% to test it later (Test)
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y_encoded,
    test_size=0.2,
    random_state=42 # This 'seed' ensures we get the same split every time we run it
)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# 9. 'Normalize' the data: Make sure small numbers (like 0.5) and big numbers (like 80) 
# are on the same scale so the model doesn't get confused.
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 10. Create the Logistic Regression 'Brain'. max_iter=5000 gives it more time to think.
model = LogisticRegression(max_iter=5000)

# 11. The 'Fit' step: The model studies the training data to learn the patterns.
model.fit(X_train_scaled, y_train)

# 12. The 'Predict' step: We give it the test data (without the answers) and see what it guesses.
pred = model.predict(X_test_scaled)

# 13. Check the score: What percentage did the model get right?
from sklearn.metrics import accuracy_score
accuracy_score(y_test, pred)

# 14. The Confusion Matrix: Shows exactly how many 'Alive' were mistaken for 'Dead' and vice-versa.
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)

# 15. The Classification Report: A detailed 'Report Card' showing Precision and Recall.
from sklearn.metrics import classification_report
print(classification_report(y_test, pred))

# 16. Create a new model that gives more importance to the minority group (the 'Dead' class).
model = LogisticRegression(
    max_iter=5000,
    class_weight="balanced"
)

# 17. Train this new, fairer model.
model.fit(X_train_scaled, y_train)

# 18. Get the new predictions.
pred_bal = model.predict(X_test_scaled)

# 19. Print the new report card to see if it got better at finding 'Dead' cases.
print(classification_report(y_test, pred_bal))

# 20. Get the 'Importance' scores for every column (Age, Tumor Size, etc.).
feature_importance = pd.Series(
    model.coef_[0],
    index=X_encoded.columns
).sort_values(key=abs, ascending=False) # Sort them from most important to least.

# 21. Show the top 10 things the model looked at.
feature_importance.head(10)

# 22. REMOVE the 'cheating' column (Survival Months).
X_no_leak = X_encoded.drop(columns=["Survival Months"])

# 23. Split the data again with the new 'clean' features.
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X_no_leak, y_encoded,
    test_size=0.2,
    random_state=42
)

# 24. Re-scale the data (put all numbers on a scale of 0 to 1).
scaler = StandardScaler()
X_train2_scaled = scaler.fit_transform(X_train2)
X_test2_scaled = scaler.transform(X_test2)

# 25. Create a final model using the clean, non-cheating data.
model2 = LogisticRegression(
    max_iter=5000,
    class_weight="balanced"
)

# 26. Train the final model.
model2.fit(X_train2_scaled, y_train2)

# 27. Predict and see the REAL performance.
pred2 = model2.predict(X_test2_scaled)

# 28. Print the results. (Note: The accuracy will be lower, but this model is more HONEST).
print(confusion_matrix(y_test2, pred2))
print(classification_report(y_test2, pred2))
